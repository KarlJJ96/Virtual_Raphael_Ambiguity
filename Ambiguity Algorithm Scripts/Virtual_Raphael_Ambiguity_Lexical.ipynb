{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1TkRE6L9YJt"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_pos_tags(text):\n",
        "    doc = nlp(text)\n",
        "    pos_tags = [(token.text, token.tag_) for token in doc]\n",
        "    return pos_tags\n",
        "\n",
        "def get_wordnet_pos(spacy_tag):\n",
        "    if spacy_tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif spacy_tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif spacy_tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif spacy_tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def extract_top_synsets(word, pos_tag, top_n=3):\n",
        "    wn_pos = get_wordnet_pos(pos_tag)\n",
        "    if wn_pos:\n",
        "        synsets = wn.synsets(word, pos=wn_pos)\n",
        "    else:\n",
        "        synsets = wn.synsets(word)\n",
        "    return synsets[:top_n]\n",
        "\n",
        "def encode_sentence(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state\n",
        "\n",
        "def get_sentence_embedding(sentence):\n",
        "    embeddings = encode_sentence(sentence)\n",
        "    return embeddings.mean(dim=1).squeeze().detach().numpy()\n",
        "\n",
        "def get_definition_embedding(definition):\n",
        "    return get_sentence_embedding(definition)\n",
        "\n",
        "def calculate_similarity(embedding1, embedding2):\n",
        "    return cosine_similarity([embedding1], [embedding2])[0][0]\n",
        "\n",
        "def process_text(text, threshold=0.5):\n",
        "    pos_tags = get_pos_tags(text)\n",
        "    ambiguous_sentences = []\n",
        "\n",
        "    for i, (word, pos_tag) in enumerate(pos_tags):\n",
        "        synsets = extract_top_synsets(word, pos_tag)\n",
        "        if len(synsets) > 1:\n",
        "            sentence = ' '.join([w for w, _ in pos_tags])\n",
        "            word_embedding = encode_sentence(sentence)[0][i].detach().numpy()\n",
        "\n",
        "            max_similarity = 0\n",
        "            for synset in synsets:\n",
        "                definition = synset.definition()\n",
        "                definition_embedding = get_definition_embedding(definition)\n",
        "                similarity = calculate_similarity(word_embedding, definition_embedding)\n",
        "                if similarity > max_similarity:\n",
        "                    max_similarity = similarity\n",
        "\n",
        "            if max_similarity < threshold:\n",
        "                ambiguous_sentences.append(sentence)\n",
        "\n",
        "    return ambiguous_sentences\n",
        "\n",
        "def read_text_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def write_ambiguous_sentences(file_path, sentences):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for sentence in sentences:\n",
        "            file.write(sentence + \"\\n\")\n",
        "\n",
        "# Example usage\n",
        "file_path = 'your file.txt'\n",
        "text = read_text_file(file_path)\n",
        "ambiguous_sentences = process_text(text)\n",
        "\n",
        "output_file_path = 'Potentially Ambiguous Sentences.txt'\n",
        "write_ambiguous_sentences(output_file_path, ambiguous_sentences)\n",
        "\n",
        "print(f\"Ambiguous sentences have been written to {output_file_path}\")\n"
      ]
    }
  ]
}